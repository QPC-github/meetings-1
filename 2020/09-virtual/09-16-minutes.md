# Privacy CG F2F, 16 September 2020

[Agenda link](https://github.com/privacycg/meetings/tree/master/2020/09-virtual)

*   Chair: varies throughout the day


#### Introductions
See self-written intros for each participant at the end of the minutes.


#### Storage Partitioning
*   Discussion lead: Anne
*   Scribe: Arthur

*   [Clear-Site-Data for partitioned storage can be used for cross-site tracking](https://github.com/privacycg/storage-partitioning/issues/11)
    *   Anne: On July 23 I put in a comment to bring to the attention to the group, and there hasn’t been any follow-up. It’s a header that allows the website to clear storage data, localstorage, indexeddb, Service Workers. Allows for clearing cookies, clearing caches. Allows for reloading execution envs in theory, though not in practice. Part of the issue here is how clear site data should work for cookies, storage and caches in light of state partitioning.
    *   Anne: There are multiple options discussed in the thread. Where I have come down, within a document, if that doc is partitioned and it has the header set, it would only clear the partition data for that document or partition. If it’s a top-level document that is non-partitioned, it would only clear the partitioned data for that document and it wouldn’t clear data in the sub-tree. Minimum amount of storage that needs to be cleared. Caches are problematic -- and that was already problematic in the original Clear-Site-Data. This would be more granular. Not sure if we really need cache to begin with as a feature. In part my proposal is that we don’t do reloading of execution contexts, we don’t do clearing of the http cache, because it doesn’t really match the architecture of how the cache is allocated and stored. There is a weird mismatch where you can go through a subset of the partition but you get really clumsy operations. So I wonder if you really need the functionality at all. Was this clear enough?
    *   John Wilander (Apple): We have talked on other threads that partitioned storage should or could be ephemeral and it is the case of Webkit’s implementation. Wonder if that plays in somehow -- Clear-Site-Data all the data is ephemeral -- does that make it less important somehow?
    *   Anne: When you say ephemeral it’s about a week?
    *   John: Doesn’t have a timer, it’s in-memory only. Eric Lawrence from the Edge team wrote a blog about it: other browses persist session data or session cookies. Not the case with webkit everything that’s session based or ephemeral is in-memory only. After you close the device, everything goes away.
    *   Anne: If for third parties, storage would always be ephemeral, and guess you could not support Clear-Site Data for third parties. I hope that’s not where we end up for third parties.
    *   John: That’s a conversation to be had. Someone on the Chrome team (Michael K) said ephemeral sounds pretty good for partitioned. We are all envisioning a way for giving third party access to storage.
    *   Erik: On the cache discussion and execution context -- do we have a good discussion on how website authors are depending on clear-site data, e.g. bad service worker push. Would this suggestion impact that?
    *   Anne: SW would not be impacted, but SW are part of storage, including Cache API in SW. Cache API in SW is a storage API. Cache thing that Clear-Site-Data is exclusively the http cache. Maybe some related network state. Nothing at the API level.
    *   Josh: In regards to the cache, I need to follow up with engs, before partitioning it’s not something we can support very well. Once we have the partitioning, it would be a reasonable operation to clear everything in a partition. I agree with Anne that we should do it by partition and not everything else on that page. In terms of ephemeral, I don’t remember anyone on Chrome I don’t remember anyone saying that is what we intended or agreed on. I think it might hurt some use cases where someone wants to store third party info but doesn’t want to share with first party.
    *   Anne: I think what you’re saying is if the top-level does clear-site-data in the cache, then you could clear the entire tree in its partition. Wouldn’t necessarily work of one of the frames did that. If A embeds B and B does Clear-Site Data, you don’t want to clear the cache for A.
    *   Josh: That’s right. Chrome is contemplating partitioning both by the top frame and the current frame, so you would just need to clear that data.
    *   Anne: I’m also concerned about the top-frame being able to influence the caches of its subframes. Sub frame thought it was in one state and then the top frame affects that state, and sub-frame goes down that path.
    *   Josh: It’s a poorly supported API in Chrome.
    *   Anne: It’s also a hack in Firefox .Would rather get rid of it until we have better use cases for clearing the cache and figureI’ll  out what better use cases we have.
    *   Josh: It’s very expensive for us as well. We have to enumerate everything in the cache to delete. So we don’t want headers to do that because performance would be abysmal.
    *   Anne: It’s a DOS attack
    *   Valentino: I want to understand the cross-domain nature of this. Shopify is an example, but you can see in many hosted blog platforms. Automattic and Wordpress do. How would clear-site-data work in the case where there is this kind of relationship?
    *   Anne: You have to get the other domain to also serve the header. One origin cannot make statements for the other origin, except in the case of cookies, which are scope to the site boundary. Everything else is origin bound.
    *   Valentino: Haven’t seen a lot of in-depth docs on clear-site-data.
    *   Anne: There’s an existing standard and we’re trying to figure out how to adapt it to a world with storage partitioning.
    *   Charlie Harrison: If I’m a top-level site (A.com) and I use clear-site-data, then it will clear origins in third-parties?
    *   Anne: Only the storage they would have access to.
    *   Charlie: My one concern is, if I am A.com and I had a user navigate to a page that said I am going to clear all the users data, do we feel the proposal make this difficult because the top-level site doesn’t have a way of easily clearing all the partitioned data. If A.com logs a bunch of data on different sites, then it can’t fulfil the promise of clearing the data for that one particular user. I wonder if there are subtle implications there with respect to data deletion.
    *   Anne: All of the data is hosted on a single origin, but the origin is sometimes partitioned.
    *   Charlie: Yes, A/com is sometimes a 3rd party. But clear across sites could also be a side channel. Damned if you do, damned if you don’t
    *   Anne: I think the side channel is worse. We want a model where the site is in the address bar. Try to model the web along that architecture. I can see there might be some tricky implications.
    *   John W: In our implementation decision to use ephemeral for partitioned storage, this kind of played into it. You’ll have these residues that’s partitioned along a great many websites for popular third parties serving content. Completely out of sync with what they see as a first party. Hey, let’s make sure it goes away periodically when you quit browser or restart the device. I filed issue #11 explaining exactly how it can be leverage for cross site tracking. (This issue under discussion.)
    *   Erik: I agree with Charlie that there may be some other important use case under data management UX piece. Side channel seems worth. Something we should think about more. New primitive? Or user education, you need to do this action through browser UI. Pretty confident that someone will comment and say this is important to them.

*   [Expose the first party to a partitioned third party](https://github.com/privacycg/storage-partitioning/issues/14)
    *   Anne: We discussed this in the telcon a while back. It’s about exposing the first party to partitioned third parties. Or third parties in general. We are wondering if we could explore that a bit more. Tanvi brought up the case where if you have A.com embeds B.com and B.com embeds C.com in turn, and A.com might be fine with sharing its identity with B, but not OK with sharing its identity with C.com. What do folks think about that? In general, if there is more input on this issue and how valuable it is for content producers to have this information, on who their first party is?
    *   Michael Kleber: At a philosophical level, I find it unpleasant to think about the state of being partitioned being inherently impossible to know what partitioned you are inside of? The data is a key value store but you’re not allowed to look up the key where you are looking something up. I was involved in discussion with ITP about the difficulty of partitioning cookies where you couldn’t tell if a cookie was partitioned or what partition it belonged to. I think there’s a lot of benefit of exposing the first party domain to show what partition you are inside of.
    *   Erik: Is this describing a new header to make sure every request carries this context?
    *   Anne: The proposal in tht telcon was maybe a header, and at the API level it should be queryable. If you have the former you can make the latter yourself, but the browser might as well expose it for convenience.
    *   Josh: On some browsers, there’s ancestor origins you can figure out what your top origin is. I worry if a frame is embedded, it might want to know it is being embedded and say I don’t want to be embedded on this site. It might make a lot of sense at the header level.
    *   Anne: I think currently Chrome and Safari expose the first party already through Ancestor Origins. Definitely in pages and maybe in SW. Firefox still exposes neither. You might be able to query some stuff in Firefox by clever use of frame ancestors in CSP, but that would be a more exhaustive search rather than definitive ancestors. We do plan to patch at some point but it’s not been a priority. Also curious about Apple’s perspective because they also have been shipping Ancestor Origins, if they are also thinking of reducing along similar lines.
    *   John: We know about this, it is kind of a legacy thing. Someone filed an issue way back . We have been thinking about it. We don’t know the consequences of deprecating or obsoleting it. Don’t know how quickly or willing we would be to remove it.
    *   Anne: BZ and I came up with a scheme where the referrer policy would dictate what ends up getting exposed in Ancestor Origins, but it didn’t pick up steam at the time. Since Google and Apple weren’t keen on addressing it, we didn’t either. We’re also not too keen on exposing it.
    *   Tanvi: I was going to respond to Josh that the website can still use X-Frame-Options to decide what’s allowed to embed it.
    *   Erik: That allows embedding or not. I don’t allow one origin but another is OK?
    *   Anne: X-Frame-Options is only Deny or Same-Origin. With Frame Ancestors, you’d have to enumerate all the possible places you are embedded in. You might want to make a decision on the server instead.
    *   Erik: Not always allow it -- make sure there’s a clear signal. You may be embedded, but whoever’s embedding you do not want you to know who they are. I’d have to determine whether there are other means anyway.
    *   Anne: With the HTTP header, Michael, are you envisioning it would only be there if the storage is being partitioned? Might be hard to determine up front. You might only be able to determine one you are creating the context. Sharing who your first party is and sharing what your storage key is going to be in this header.
    *   Michael: Really good q. I was thinking of sharing what your storage partitioning key is. I think either one could work. Do you have partitioned or unpartitioned storage is a really useful signal for letting servers work with this maybe surprising state.
    *   Anne: I was confused, because you definitely have to know it up front because of cookies.
    *   John: We’ve been doing partitioning for 8 years. It’s been in our toolbox ever since. More and more it’s becoming stateful down in the network stack. HTTP3 state. DNS Caches, TLS sessions and resumption of those. Is this the right place to talk about those things? Or do we need to spin up an IETF sibling effort?
    *   Anne: I think for now we should talk about them here. I was hoping that over time that it was address these mostly in the Fetch standard, in a similar way that the Fetch standard encapsulates the HTTP cache. We could do similar things for DNS and other things. You get your DNS subsystem from this algorithm, and the algo keys it on various things. We have enumerated all this network state already in the storage partitioning document, but we haven’t turned it into prose yet. Definitely in scope. Don’t see that there is much need for IETF side involvement unless we find a place where it gets really unclear.
    *   John: And the rare cases where things are security related: HSTS and HPKP. There was a recent discussion on the W3C Web-App-Sec where we were talking Artur Janc from Google about something similar to CSP a site could say -- for my whole origin apply these protections. I think it was opting out of legacy tech. Please apply to my whole origin. This becomes state.
    *   Anne: Ideally if we do new technology we take this into account from the start. Maybe you are talking about Origin-Policy?
    *   John: I’m just thinking about cases where it is about security. I’m thinking about HSTS. We set HSTS to deal with TLS situations, but if we partition, then there is this trade off (between sec and privacy). Do we partition all of it?
    *   Anne: For HSTS the thing that seems most promising is if we can upgrade all mixed content. Only media can be mixed content currently, but if all mixed content is upgraded, we don’t have to care about third parties.
    *   John: Except when the first party is HTTP, which means that the third party loaded over HTTP will not be mixed content. So HSTS could have helped them but it won’t. The attacker will deliberately move the user to a non-secure page. Not for us to solve here, but it is an interesting tradeoff. Not just work and compat.
    *   Anne: That’s fair.
    *   Josh: Two things: (1) We (Chrome, Matt Menke) are actively working with Anne in terms of spec and currently writing up an explainer. We have partitioned socket pools, DNS, HSTS, all the things. The V8 code cache. We’re talking about -- favicon cache also needs to be double keeyed. Site Isolation also needs to be double keeyed. We have a list of a hundred things. Want to talk about -- what is the partition? We have been playing with is top-frame “site”. We have also been playing with notion of top-frame “site” and “current frame”. If we just key by the top frame site, all the frames can snoop on each other. If we key frame top-frame and current site, then individual frames should not be able to snoop on each other. Although Anne and others have pointed out there are side-channels leaked. We are launching on cache in the near future. Would make sense with compat to go with the top site. For security purposes, I think it makes sense to isolate individual frames. It should also apply across the board to sockets as well.
    *   Josh will create an issue 
    *   Anne: We are interested in exploring that. For now we have decided to just focus on top-level sites, because that’s a big security win over the status quo. Hard to reason about properties of additional keying. We considering for a while each frame adds a key. Even if you have top-level site, plus your current thing, you can not attack your parent, but you can attack your sibling still. Some attacks were still possible. You couldn’t attack your siblings but you can embed siblings. There’s already a number of attack scenarios on embedded websites. It wasn’t entirely clear if solving the cache problem would move the needle enough. We are definitely open to exploring that and adding more keys in the future. I think it needs some more analysis.
    *   Josh: Getting a position on what you are planning on doing helps to.
    *   Kaustubha: Follow up question about the fact that Firefox was not sure about supporting the Ancestor API. What’s the reasoning behind that. Is there an attack we’re worried about?
    *   Anne: I’m trying to remember the exact scenario. I don’t think it was an attack but it was that we didn’t want to leak that information to third parties, what the user was currently visiting. Suppose there’s a popular third party that embeds video. We wouldn’t want that video embedding website to know all the sites the user is visit. We don’t want to have that necessarily. To some extent this would already leak through referer, but if the top-level site did the extra step of not leaking its referrer, then it should definitely not leak that data by other means.
    *   Kaustubha: I think that definitely makes sense. I think we were talking about this being in a partitioned world. Is that sort of attack. If that embedded site has partitioned storage, do we imagine a way there would be for a third-party site to correlate to a users visits across origins?
    *   Anne: We haven’t solved that, there's still IP address, and we also haven’t solved fingerprinting which is pretty tricky. 
    *   Erik: I have put a link in the slack -- ancestor origins and referrer policy. A refresher there would be interesting to see. Do those concerns change with partitioning?

#### Storage Access API
*   Discussion lead: Tess
*   Scribe: Melanie
*   [Make implicit deny and explicit deny indistinguishable](https://github.com/privacycg/storage-access/issues/60)
    *   Tess: John Wilander, you filed this. Can you walk us through?
    *   John: This is about Storage access API, 3p is requesting and is denied. Can happen in 2 ways, at least in Webkit, assume same for other implementations. Either policy says, this 3p cannot request storage access. Maybe user has not been to site, not interacted with site, or no cookies to get access to. Because that happens immediately, the promise for storage access is rejected immediately. Whereas if the user is prompted, it’s going to take a little time. Users take a second or two to reason about the prompt. Time difference enables 3p to detect whether they were denied access immediately or whether the user denied it. Complaints for users with 3p embedded video, the video starts and then they’re prompted for storge access. If they don’t choose anything, nothing happens, video plays. IF they deny, the video stops. Users feel punished or pressured to opt in. Can we make it so that these denials are indistinguishable so that the 3p can’t give a different experience based on it?
    *   Tess: the pausing behavior doesn’t happen if we auto reject?
    *   John: yes, but might have gotten vague reports. Original reports were that if there is no prompt, the video plays fine. Only in cases where the user says don’t allow, the video stops. Issue: should we/can we do this to make them indistinguishable?
    *   Tess: inject a time delay?
    *   John: yes, mentioned in issue. Also the idea of hanging. If it’s blocking by default, you don’t have to reject the promise if the user chooses don’t allow, you can let completion handler hang. It’s as if the user never chose anything, which is a valid case.
    *   Tess: Instinct is it’s weird for platform API to say, I’m going to return a promise and it never gets resolved. That said, sounds like sort of thing you’d expect in interventions spec. This is an intervention the browser had to do to get around some sites.  Not outside realm of impossibility,, seems a little strange.
    *   Michael Kleber: not that strange if you think about it as, I’d like you to fire an event if thing happens. That event might not fire. I wouldn’t worry about philosophical implications
    *   Michael Kleber: ...for the original problem John described. I have no objection to making this change, seems fine. But it seems if someone wants to exert pressure, it’s not going to be a problem for them to continue doing so even if you make this change. No objections, but doesn’t seem like it will help w/ people exerting pressure you don’t like.
    *   John: many things come to mind. Maybe right that there’s other ways to pressure/punish. Are you thinking of pre-prompt, and only if you say yes to that prompt will I show you the other prompt, or stopping video unconditionally if you don’t get the answer you want?
    *   Michael: seems like a wide open field, I don’t want to try to predict [bad site behavior]
    *   John: when I as user say I don’t want to grant access, and the video stops, now Safari hasn’t done the right thing for me
    *   Scott Low: I wanted to echo Michael. This problem doesn’t seem unique to storage access API. True for any promise-based API, e.g. geolocation. [distinguishable explicit no vs immediate rejection]. I’m not sure if there’s something we want to do to fix this issue, a reject is a reject, if the site wants to pressure either way still possible.
    *   Ben Savage: I’m not convinced this is a problem that needs to be solved or is nefarious. I can think of a reasonable use case when you’d want to understand implicit vs explicit deny. You want to understand what % of the time, when presented with the prompt, users say yes or no. What are people doing? To understand engagement with embedded content. If some % were never shown the prompt and never had a choice, you’d want to exclude that from the data. This seems like a normal part of understanding interaction with content. Changing what experience you offer people given rejection, I don’t think this is necessarily nefarious. Even in the original explainer, it talks about legit scenarios for offering differentiated experiences. [granting user more access to content] It’s about the value exchange between the website and the customer, you need to have a value exchange, I don’t think preventing/interdicting on value exchange is required.
    *   Jack Frankland: another use to make it indistinguishable? Is there a potential tracking vector? The prompt alone will only say whether the user has interacted with the site in a 1p context. Clicks in iframes, could you know they’re a particular subset of user?
    *   John: there could be such a thing, I think so far we’ve felt that necessarily click in 3p content is not enough to be scalable, but there is potentially fingerprintability there. For Ben’s comment, I would tend to agree if it weren’t for the fact that the video is actually playing under the prompt. The site is showing that they are willing and able to show content w/o cookies. But I will deny you if you don’t allow cookies. That is what people are complaining about...clear to user that there is a value exchange required by the 3p. [looking for an issue] I think it’s [55](https://github.com/privacycg/storage-access/issues/55), it talks about registering an event handler. Goes to what Michael talked about. Maybe that’s how to resolve this. Tell me when I get access. That will fire when storage is granted, but nothing happens when storage is not granted. Now [implicit/explicit] is indistinguishable.
    *   Josh Karlin: I agree with the notion of not necessarily notifying, I saw research that users often ignore prompts, so these would be hanging. Strong similarity with what Google has been dealing with regarding notifications. Sites forcing users to accept notifications, often abusive, they’ll use the notifications for abusive advertising purposes. What we chose for notifications was fairly involved. We started tracking sites where users were not accepting, give those lesser UX treatment, if it appears abusive, we may just drop the notification request.
    *   Ben: in response to John. I think your complaint is a broken UX where a developer created an experience on the internet using basic primitives on the web and you don’t like it. There’s always going to be broken UX...I don’t think any browser can possibly achieve not creating broken UX on the web. I don’t think browsers should police this. Sisyphean task.
    *   Tess: general thing with browsers, if sites are broken, they blame the browser, not the site. They try a different browser and it works. That’s an ecosystem force that means browsers make a lot of changes over time. When I say broken, I don’t say code has an error, I mean from the user’s perspective. To push back a bit on what Ben was saying, I think it is the responsibility of the browser given ecosystem of competition, to make sites work well. Sometimes that’s fixing bugs, sometimes that’s working around site behavior.
    *   Anne van Kesteren: another potential problem, user denies, sometime later the user visits again. One scenario, sometime later, the storage is gone. Other is sometime later, the storage is still there. But user has stored the user decision. In scenario when storage is gone, where we reject directly, we give a bit to the site where this is a return user...I do care about being able to identify a recurring user. That’s the thing we’re solving here, it’s separate from UX concerns, it’s a basic tracking concern.
    *   John: I tend to agree. Both are valid concerns.
    *   Ben: re Tess, I understand users have choices on browser they want to use. If a site doesn’t work in one browser, that forces competition between browsers, and I think that’s a good thing. If [a browser makes a decision] that makes a site not work, that’s a healthy dynamic that helps users decide what browser they want to use. Going back to value exchange. If browsers break the value exchange, then we should expect users to be pushed towards places where they can make the value exchange.
    *   John: keen on exploring hanging on reject, and potentially offering an event to be fired when storage is granted. So sibling iframes can know if an iframe asked for storage and got it. [missed the last sentence]
    *   Anne: isn’t problem with hanging that you can identify a recurring user?
    *   John: I don’t think so, hang all the cases
    *   Anne: the first time they visit though?
    *   John: never reject the promise resolve, or hang
    *   Anne: oh I see
    *   Michael: for visitors who say yes, you could see if they’re recurring. But if they said no, indistinguishable.
    *   Tess: seems reasonable
    *   John: we do have a case where reject was a way for the embedded content to show a popup so the user can be logged in. We’ll have to think about that, we might be breaking that with the hang.
*   [Require reload after granting requestStorageAccess to get at unpartitioned storage](https://github.com/privacycg/storage-access/issues/62)
    *   Josh Karlin: newcomer to the API, so there’s things I’m learning. At first blush, I get concerned about browser getting transition right from partitioned to unpartitioned. I also worry about dev experience. I think that is a nightmare for browser devs and publishers. Talking about multiple-day grants, you could say ok that frame has to load if wants access to storage, but that’s a limited use case...I agree reload is not ideal. If you don’t want to reload, you can create an invisible iframe. Informed not a good idea, all the frames we’re talking about are all partitioned or unpartitioned at the same time. I’m confused, does that mean iframes will switch between partitioned or unpartitioned and not know what’s going on?
    *   John: early on in storage access, we had a rule in Webkit...WK is still shipping per-iframe access, though spec is per page. In early implementation, it would revoke storage access on frame navigation. Dev feedback, I need to re-render after I get storage access. So we made change, same site navigation is allowed, and storage access will be persisted. If it moves cross-site, we would revoke for 3p that was granted. That model would fit fairly nicely in what you’re thinking about reloading. We’ve talked about bridging partitioned and unpartitioned. Haven’t thought about as super bad thing because our unpartitioned storage is ephemeral...all goes away if user quits browser or restarts device. We’re now in situation where, we agree to have per-page access, and that would make this whole thing more complicated. What happens in sibling iframes with ongoing transactions? I think Moz best to speak to that because they’re the ones that have experience with that...our hope is that any new connections with IndexedDB would be unpartitioned, existing ones would stay, but agree could be very weird.
    *   Anne: AFAIK, we do the transition live between partitioned and non-partitioned live, across the same origin within an agent cluster. Not much of a problem, but if other iframes have ongoing transactions, yes, they would be impacted. In our implementation we have managed to make that work. Basically the synchronous script access boundary should match the storage boundary.
    *   John: you block based on the list you have in ETP, no issue until granted storage?
    *   Anne: yes, hope to give all sites partitioned storage, and if they revoke the API, unpartitioned. Blocking would be equivalent to having partitioned access...still isolated from everything else
    *   Josh: the per-page thing kind of perplexes me, want to understand reasoning. Sounds like you’re saying all will flip over, requester will have some idea, the others won’t. The proposal for an event will help, without the event, chaotic...how would 2 frames talk to each other to say hey I just got storage?
    *   Anne: grant is only for one origin, so only same-origin frames would have access
    *   Josh: ok fair enough, if they’re different frames from same origin, that can be really complicated. Looked at your data, not actually that many prompts. Looks like this hasn’t been tested on a large scale of sites. Any insight into how publishers feel about transitioning in the document?
    *   Tanvi: let us get back to you. Part of this, we’re experimenting with partitioning. There’s only a number of web compat issues surfaced thus far.
    *   John: it’s not just frames. Per-page means all subresources for that site. That only matters for cookies. Rest needs JS in an execution context, iframes. So all storage gets access. It sounds as if you think a reload of all sibling iframes, after one of them gets access, is very bad. I’d like to know why.
    *   Josh: IDK how many frames someone would have. Let’s say you reload 4 and have a flash of white for 4 things at once, it’s just a bad experience. As simple as that. I think devs would be frustrated if we asked them to do that and didn’t given an alternative option.
    *   John: we know of cases where there’s very many cross-site iframes from a single provider on a single site. Reloading all iframes presents opportunity to gate site isolation. Resource constrained situation, e.g. mobile device, I’m only going to do this when it’s highly likely personal data in the process...we could say, now this embedded content needs a separate process, I can spin that up, and reload all the iframes. That can potentially resolve partitioning of site isolation.
    *   Josh: I could see that as a nice way to do that
    *   Anne: saw another issue with promise-returning method to tell you when doc had storage access, that seems helpful. Everyone would get a ping when access granted.
    *   Josh: what if one of the frames isn’t expecting 3p access at all? But its whole world change on it?
    *   Anne: I think that seems unlikely
    *   Josh: origins that have many different departments…
    *   John: a lot of the concerns you bring up Josh are discussed in issues where we land on per-page access. Webkit, we still sort of want per-frame, but we were convinced by Mozilla and MS, mostly for compatibility reasons, it would be better per-page. Scenarios built under assumption of 3p cookie access always. Want to have a step back towards the old world...a lot of developers aligned with Mozilla and MS on this.
    *   Josh: that’s important, I think it’s harder for browser to do per-page, so I want it to be worth it. I’d like statements from 3p developers.
    *   John: it should be captured in issues, we can help you find it. It’s important, especially if we revisit.
    *   Tanvi: let’s find and give it in the notes - [https://github.com/privacycg/storage-access/issues/3](https://slack-redir.net/link?url=https%3A%2F%2Fgithub.com%2Fprivacycg%2Fstorage-access%2Fissues%2F3&v=3)
    *   Josh: last I saw, request storage access spec, there was a hole in how do we do the merging business. More going on there, behind the scenes discussions? No discussions yet?
    *   John: what is merging?
    *   Josh: the transition, what do we do with Service Workers, IndexedDB?
    *   Anne: Idea I had, not formalized, a similar transition to clearing data. Replacing with empty bucket, this would be replacing with 1p bucket. I don’t think we specified how clearing data works, so I’m trying to look at the first. Maybe some central API that calls out to other APIs that need extra work done...straightforward at the conceptual level. For some APIs need additional work, canceling transactions or letting them going on. Their output doesn’t matter anymore because new transactions will act on the new data store. That needs to be detailed, haven’t gotten time to flesh out more. There’s an issue against storage to work on fleshing that out. I would expect storage standard to provide the overall infra on how to transition, and storage access API would provide signal on when to flip.
    *   Jack Frankland: I think this was based around in Safari, all storage blocked. And so preference for devs is that once granted, all frames are unblocked
    *   John: I assume you mean cookies, not storage.
    *   Jack: I think the interest is that all the cookies would be unblocked per page, and the developers would want that to be global.
    *   John: interesting, maybe something to explore. Cookies are the per-page case, typically carrying auth info. And STORAGE could be per frame. Moz? MS?
    *   Brandon Maslen: our initial push for per-page was due to same thing Moz has, we block everything first, going from block to unblock state. Here talking about cookie not getting the same partitioning makes a lot of sense. May make sense where we have per-page scope, but the actual transition happens per frame. Some requests storage, we will partition new frames after they’ve loaded
    *   John: on to something interesting. Maybe we’re converging on: partitioned storage stays partitioned if it’s an iframe that didn’t request storage access. Anything blocked before storage access gets unblocked, and gets unpartitioned storage access. Iframe that requested access can get the transition. Maybe too complicated for devs?
    *   Josh: decision was made when we were in the no-access to access situation.
    *   Josh: when request storage access resolves, we’ve been talking about wiping out top level and replacing with unpartitioned. Could return object with access to partitioned, not top level. Or we could return cookie string, like document.cookie, if we don’t want to do entire object. Again could reload...I think per-frame is better than thinking about per-page
    *   Aram: I did like idea Josh just subscribed, ability to make request, get something back as part of that request, use it from there, makes sense from dev flow.
    *   Anne: per-frame vs per-page, say you have embedded doc with partitioned storage. It may have its own subframes. They can invoke script in each other. Currently storage is global and same in all these frames, doesn’t matter who you request storage on. If we do swap, it suddenly matters what local you’re invoking storage on. That’s a concern with different boundaries for script and storage. That’s why we like them being tied together...For me, that is a good reason to try not to tie storage to a document. I worry about compat, I know for sure there’s libs that create frames and try to sandbox things there.
    *   John: I believe Tess brought up IndexedDB issue, async connection. What should happen once 3p transitions from partitioned to unpartitioned. Similar to registering an event handler, tell me when I get storage access. We could use that as an opt-in signal from the other iframes. We could just sever your connections and switch you to unpartitioned because you asked for it. The others can stay in old state. The exception is cookies, they’re a protocol thing, they go from blocked to unblocked.
    *   Jack: there’s an exception to that rule where there’s a popup, do have immediate access to both partitioned and unpartitioned. Comms between popup on the 1p and the iframe. In terms of storage the embed might want to get. I think cookie might be enough. What’s the use cases for embeds to use other storage like localStorage and IndexedDB, what’s the likelihood they want access to that and not just cookies? Like Josh said, they can resolve state themselves. Another, what if you got access to a new window or worker window that had unpartitioned, would you then message to get to that state?
    *   Tess: I think we can continue this one on the issue. Really good discussion. Really helped me understand what the issues are.

#### First-Party Sets
*   Discussion lead: Kaustubha
*   Scribe: Ben Savage
*   [Desirable elements of "UA Policy"](https://github.com/privacycg/first-party-sets/issues/20)
    *   Kaustubha: We discussed this a while ago. Great feedback. Links to prior art. FTC guidance and DNT definitions were useful on topic of UA policy.
    *   Ownership is a part of it. Some comments to address, but this is specifically about UA policy. We didn’t hear from the other browsers yet. Safari / Edge support is predicated on condition that this is in the user’s best interest and understandable. Would like to hear from them what would be a deal-breaker.
    *   What are the core elements important for “in the user’s best interest”.
    *   We will have to work with policy experts to iron out the “sharp edges”.
    *   Extended validation certificate shows ownership. There is a little bit of precedent. Would like to focus on principles. 
    *   Other comment was about ICANN - responsible for DNS. Disagree because they are responsible for DNS system. The reason we are thinking about this is assuming the boundary is defined by the DNS. That’s outdated. This is an orthogonal idea. It’s more in the “application layer”. 
    *   Arthur Edelstein: It’s hard for any policy to be objective. It seems it’ll inevitably be subjective. There’s different interests from different groups. USers want privacy protected, corporations want more information. There will be debate and conflict. It will be difficult to figure out who gets to make these decisions. To me, there is no policy that can really work for everybody. If we can get an objective policy, that’s simpler for everybody involved. Especially as this is for user agent, we should focus on user interests, not corporate interests.
    *   Paul Bannister: Agree with Arthur. Potentially one more objective way to link domains together is shared privacy policies & practices. Users can understand that their data is being used consistently and coherently. That feels good and objective.
    *   James Rosewell: ICANN are at least a stakeholder in this. Should not progress without their involvement. A paper: “[Tussle in cyberspace](http://groups.csail.mit.edu/ana/Publications/PubPDFs/Tussle2002.pdf)” mentions this particular problem in relation to domains. That seems relevant. The W3C doesn’t have to solve every problem on the internet; why is this a problem to solve here? Ultimately, this is a policy question, not an engineering question. This is still an abstract question, recommend dealing with policy first, prior to engineering.
    *   Erik Anderson: Speaking for Microsoft, I agree that ICANN is not a significant stakeholder here. I think it will be challenging to define this policy, but it affects actual platform behaviour.
    *   Some concerns about interop. If it affects core platform behaviour.. We should make sure browsers do not get into a broken state. I think we are really concerned without something like first-party sets that sites will be incentivized to join different properties within a domain. Multiple, potential negative effects on end-user-understanding. It would be unfortunate if a security issue on one site then impacted others. I don’t want to discount the potential negative impacts of this. 
    *   From a UX viewpoint, it’s an interesting question: can you build something that people will understand? URL is still pretty important. How do users understand what site they are on, but also understand what other sides data is shared with. Technical enforcements on the sizes of these things. Can we detect bad behavior and penalize domains?
    *   John Wilander: We’ve expressed that we are interested in this, but we haven’t seen a viable path to something that will work out. We want to work with you to get to a good place here. We would like these sets to be fairly small. We hope we don’t wind up with 10s or 100s of domains in these sets - users will not understand. 
    *   We’d like to see purposes. Owners state the purposes of these domains. Key use case: single-sign-on. E.g. accounts.google.com -> this is my SSO domain. Then we could have reasoning around this both for storage access API and isLoggedIn. Then we can communicate with the user around authentication. 
    *   We originally thought about changes. Companies get acquired, domains decommissioned, etc. That would push us into a dynamic system. On the other hand, stability is important for browsers to know when to do things. Sets should be “cacheable” for some period of time. Specifying in the set “this set is viable for 30 days from today”. The owner cannot expect to push changes faster than that. 30 days is drawn out of the air. 
    *   Finally, there is a branding aspect of this whole thing. We know a bunch of site owners register URLs for new product launches, etc. That has in some part created the need for something like first-party-sets. Can we explore “branding” as a separate issue. Is first-party-sets the only way to solve this? Favicons is a different approach for branding. On the other side there is phishing. Branding is used to trick users. Can we resolve some of that problem?
    *   Sam Weiler: To address James point: also disagree that ICANN has a stake in this. Feel free to go pursue the policy questions in policy forums. But for the technical solution being discussed here, ICANN doesn’t have a dog in this fight. 
    *   Kaustubha: to respond to Arthur’s claim it’s impossible to have an objective criteria due to conflicting interests: I think there is a careful balance we can strike. Corporations also want their websites to function. With all of these privacy mechanisms we are building, there will be extra stuff to consider w.r.t. Cross domain communication. We will tell the user “hey, these sites are related, so you might see your data joined across them”. It’s a hard problem, but we can look to existing examples. 
    *   To Paul’s point that privacy policies are objective: that makes sense and was part of the DnT specification as well.
    *   To James’ point: others have answered. I agree with them. There are some proposals in the IETF (RDBD)  there are a few proposals but we don’t think they are relevant. We think the W3C is the best place to discuss.
    *   To Erik’s point: Interop issues are things we definitely want to avoid: which is why we are talking about policy issues here - to try to avoid that. John talked about companies changing hands and caching; there are mechanisms we can build in to avoid those. You bring up security being tied to the domain; I’ll leave that to the next discussion. You also talked about UI, there is a hesitance at Chrome to discuss UI in specs. We can discuss in the future. 
    *   Both you and John talked about size limits, we can discuss offline as well.
    *   John, please open an issue on the question of purpose.
    *   I didn’t really understand the point on branding. Can you repeat?
    *   John Wilander: In these conversations, it’s often brought up that companies register domains for small parts of their business for branding purposes: e.g. ‘tvnetworkgoldpackage.com”, then they leave it although it was just for a fall branding campaign. That inflates the number of domains that would have to be considered for a first party set. If we can figure out a way to support this branding use-case in some *other* capacity… You could imagine having the “YouTube” look when you’re on youtube.com in a way phishers cannot replicate. Just thinking this could help us avoid large sets.
    *   Kaustubha: Great discussion for a different issue.
    *   James: There is a group called “[Partnership for responsible addressable media](https://www.ana.net/content/show/id/ResponsibleAddressableMedia)” PRAM. Huge companies. Many of our salaries are paid for by their ad dollars. I suggest speaking to PRAM through IAB Techlab to capture their interests.
    *   Re: Sam’s point on ICANN - why not just talk to them and seek their input? They can see if it’s something they want to be involved in.
    *   In relation to this problem: [TCF2 (transparency and control framework)](https://iabeurope.eu/tcf-2-0/), its solving much of this problem and adopted by Google. Thorough policy work. TPAC seems like ideal opportunity. 3rd action could be to ask IAB Europe on TCF2. 
    *   Finally, Erik and John spoke about a sporting metaphor of a penalty box. That implies a referee. What are the rules? What if the referee is also a player? Favoring their own solutions over another? Another complex policy situation. John mentioned authentication. Another competition issue. That should be avoided as well.
    *   Paul Bannister: Size. I understand the desire to keep them small, but it’s a challenge in any scenario to make that work. Google / Apple has dozens or hundreds of domains. The country extensions make it large. Another example where this is challenging is “i heart media”. They own 1500 radio stations across the USA. Clearly one company. Likely one privacy policy. Is it tennable to keep them small?
    *   I wonder if we are trying to make this good and solve lots of issues and have protections in place, but if we make too many rules will it not be useful? Adoption concerns. Perhaps just 1-2 simple rules. Toshiba has all their country sites under the same eTLD+1, but they all have different privacy policies! Not changing often is important. Common privacy policy is important. Google has a common privacy policy, worldwide. That’s great. Let’s keep it simple.
    *   Michael Kleber: I do understand the desire to avoid large sets. I think its absolutely necessary support amazon.co.uk and amazon.com and others. If we cannot share a shopping cart across multiple country sites, that’s broken. Country specific TLDs are a clear counter case.
    *   Wendy Seltzer: Happy to bring in other audiences, participation with other organizations. We work with IAB and ICANN. I’ll take note of those and other conversations. 
*   ["first-party" token](https://github.com/privacycg/first-party-sets/issues/5)
    *   David Benjamin: (VC issues) 
    *   This topic: where would we want to stick a first-party token into protocols. One issue we ran into was this. We’ve been trying to provide access controls at the site level, as well as things the browser imposes on the site. In the site controlled thing, we tend to use origin as the granularity. This is useful because accounts.google.com and calendar.google.com are not able to influence one another. When do you use one vs. the other. 
    *   You can imagine having such a token, but there are some interesting problems. If I add a new domain into the set, because browsers implement X, Y, Z mitigation against bounce tracking… you weaken the security gains you used to have. If the site can specify the origin and knows what security rules it wants…
    *   For longtail things, we can push them to CORS.
    *   Tanvi Vyas: We filed the issue a long time ago and we still aren’t sure how to define first party. We are getting ahead of ourselves. 
    *   David Benjamin: We need to know what site we are dealing with. We can defer this to a later time.
    *   Kaustubha: I added this to the agenda because we have questions about how we envision this being used on the platform. I was wondering if this could lead to work items. John Wilander has talked about this helping with SSO. Perhaps there is some integration with Storage Access API. That’s the type of discussion I was looking for. 
    *   James Rosewell: I think the point about defining parties is an important next step. There seems to be a singular definition around 3rd party and 1st party that’s overly simplistic. Someone has different trust choices that they wish to express. Under what circumstances are those trust choices acceptable? How are they expressed? How do we ensure these choices are respected? Supply chain. Businesses benefit from banding together to achieve scale. The simplistic view of 1st and 3rd party doesn’t reflect the complexity. We need clearer definitions of parties. If you’re happy to trust a coalition of publishers and see personalized advertising in exchange for content, we should support that. If someone has a different choice we should respect that. 
    *   Kaustubha: I don’t believe supply chain relationships are affected at all with the browser privacy restrictions, because embedded 3ps are still able to get partitioned state and continue to offer their services within the context of that top-level page / 1P.
    *   Ben: I think if we make a UA policy that is objective and public, that is a baseline requirement. But it is important to vet what types of sets are then viable.  Has google done or have plans to do user research to understand what people’s expectations are.
    *   Kaustubha: yes, have thought about this.  Some useful insights.  Want to conduct more.  Planning to over next few months.
        *   From Zoom: @Ben Savage: Here's the paper I was referencing: [https://users.ece.cmu.edu/~lbauer/papers/2016/pets2016-online-tracking.pdf](https://users.ece.cmu.edu/~lbauer/papers/2016/pets2016-online-tracking.pdf).  Of particular interest are Section 4.2.3, and "Trust" under Section 4.3.2
    *   Kris Chapman: I agree with James 1st and 3rd is too simplistic. But it makes sense to me why browsers look at that. One thing I think we are missing in this conversation is the browser aspect and interpretation of this. We also don’t talk about it as a user education tool. One reason I like it, is to explain the relationship between domains and how they interact. I would actually want this ability to put in there that this domain is related as it’s held by the same company. 
    *   If we stick with the mindset of “we are stuck with 1st and 3rd party context” it’s going to be that much harder for users to understand what is going on. We should build in more ways to educate people what’s going on with different websites.
    *   John Wilander: I did bring up the idea of adding purpose to domains. It goes into this space of there being more than just 1st and 3rd parties. I assume that one domain can only be part of one set at a time (it can move of course). Does that align with this idea that there is more complexity here? Can we define the supply chain as well?
    *   Kaustubha: In response to supply chain question. Single domain, single-party multiple domains, embedded services… I see this all as something that can work in a partitioned model. I think all of the work we are doing in the Privacy CG preserves that functionality.
    *   Ben: could be used as a tool for user education.  Possibility to use it to stop phishing.  Problem I'm passionate about. A lot of deceptive domains.  Fantastic idea to have browser warn people that this thing is not connected to the legitimate domain.  Hit with phishing recently.  Great if browser told me about this.  
    *   Kris: Just to respond to John - I appreciate the purpose aspect. I was looking for more than just the purpose. Is this a partnership relationship and other things. To the question of: “Should a domain be allowed in multiple sets?” I think yes.
    *   At Salesforce we have a suite of tools where clients are logging in to administer their email campaigns. The associated tools should have a unified interface. But consumers on the other side of these tools, shouldn’t have that much access from a user facing perspective. It might just be purely about one tool. There’s an aspect of the set which is the context of how it is being used.
    *   James: I agree with Kris’s point that there are restrictions. Different legal entities are complicated. To Ben’s point, that’s a great use case that needs to be considered that goes beyond first-party-sets.
    *   On Supply chains: People should be able to choose how they want to trust the publisher. Supply chains are complex. Chain of trust is extremely important. We should seek to ensure people are informed, but place restrictions on what they are able to do. Repeats point on TCF2. 
    *   Paul Bannister: On the “purpose” topic, what are we trying to solve for? The goals of the first-party-sets explainer is overly broad. Defining at a high level the purposes are important. Why does this first-party-set exist and what can it do? One concern I have about all of this, wanting to make sure that the decisions do not advantage large organizations over small. In terms of organizational ownership, this advantages large conglomerates. This is not necessarily a good thing for users. 
    *   John Wilander: The purpose thing I brought up was per-domain, not per-set. Maybe we need that too..  I’m thinking that the SSO status should only apply to one domain. Useful for browser mediated login. 
    *   Erik Anderson: Listening to Kris’s point about Salesforce and being in multiple sets… It wasn’t clear to me that the central provider needs to share across those. “Portals” concept from Google. iFrame that can pop into a context. There is maybe some interesting thing, additional partitioning scheme. This site is working on behalf of this other site. I’m hesitant due to the complexity of user understanding. I’ll open an issue if I have something more structured.
    *   James: To pick up on Paul’s point on importance of competition and John’s on login… if the consent at the point of registration is biased, that’s problematic. When you install an essential product, you’re consenting to a lot of other services. Your use allows certain aspects of first party sets, like login…  you have a opportunity for a large company with vertically integrated products to do something other companies cannot do. There are also competition implications. 
    *   Valentino: I think I have a simple reason why it is everyone’s benefit to allow multiple set participation: analytics. Allow some company to provide analytical services to small sites. They’d need to trade in PII. That would defeat our privacy goals. 
    *   Paul Bannister: Nomenclature. It makes me naturally think of 1st party and 3rd party. That construct might not be valid in a future world. Maybe if there was a new name it would help. What does the user actually think about this? “Privacy Sets”? 
        *   From Zoom: Kaustubha - Great point, Paul! I do think our naming trips people up a lot
    *   Michael Kleber: Replying to Valentino’s comment on 1st party analytics. A company that only wants to provide 1st party analytics does not need any first-party sets involvement at all. Sites can still use 3rd parties to do analytics on them. The only time this would be important would be if the company was providing analytics on user behavior across different domains.
    
### Breakout Session on PCM/Ads
* Lead/Scribe trade-off pair: Charlie Harrison, Michael Kleber
* Attendees: John Wilander, Erik Anderson, Russell Stringham, Ben Savage, Wendy Seltzer, John Sabella, Mark Xue, Theodore Olsauskas-Warren, Andrew Knox, Paul Bannister, Christine Runnegar, Tanvi Vyas, Arthur Edelstein, Aram Zucker-Scharff, Jack Frankland, Valentino Volonghi
* Agenda topics:
   * Resolve some lingering layering issues w/ PCM and conversion measurement
   * General discussion of the TURTLEDOVE / SPARROW family of approaches to ad targeting

* Charlie Harrison: Some layering discussions, resolving differences between Chrome and Safari have happened in various issues.  Let's discuss.  Chrome's proposal includes noise: there are 3 bits of conversion-time metadata but it adds noise to those bits.  Safari's has no noise.  Can we resolve?  One possible answer: a "noise rate" that is spec'd to vary by browser, and then the conversion report has to specify how much noise was added (e.g. "conversion bits noise 5%, impression bits noise 0%" for Chrome's)
* John Wilander: In Chrome's proposal all data can be tied to a specific event; in Safari's, you can't trace to a specific event, so the need for noise wasn't particularly urgent.
* Charlie: Yes, agreed, this is why we care — in PCM there is less need
* John: We're not inter-operable on conversion value range either (your 3 bits vs our 6).  We can try to align with 3 bits, and bump up the Safari campaign ID bits, which seems like the dominant preference from advertisers as well.  We could say that the UA is allowed to apply noise, and it's an optional thing
* Charlie: Would you support adding a field in the conversion report to specify what noise the browser added, so that downstream entities understand the properties of what happened?  So that collectors don't need to do UA sniffing to know how much noise a particular reporter implements.
* John: I'd have to think about it — my instinctive reaction is to align and always supply noise, rather than add the complexity of different implementations needing to specify noise levels.  With the PCM heightened restrictions, though, not sure if Safari's version plus noise becomes entirely useless.
* Charlie: Would be great to hear from someone on the call on this!
* Ben Savage (Facebook): I have a proposal: There are two ways to dial up the privacy knob:
   * PCM style — restrict number of campaign IDs
   * add noise to whether a conversion happens at all — introduce false positives and false negatives on whether you even fire a conversion (i.e. drop some real conversions, add some fake ones for clicks that never actually convert)
* Ben: ...Could unify these proposals by altering rate of these two types of privacy-providing noise of these two types
* Charlie: We have this written up in our original explainer, as a speculative mitigation to add more privacy.  Another way to think about it, since Differential Privacy is hard to understand, is that you only get accurate data when you combine a bunch of results together, but individual reports are unreliable.  This lets you see the equivalence between PCM structure (which reports aggregates about campaign IDs) with the local DP idea, where you get event-level reports but they are inaccurate until you combine.
* Valentino: For a lot of attribution companies, the specific cookie or tracking capability is less interesting than the sequence of events that led to a conversion (e.g. some impressions, some clicks, then a conversion) — seeing the totality of the seq of events lets you allocate budget correctly.
* Charlie: This is the multi-touch scenario, which is a separate issue on the list.  Let's resolve the noise Q first, then come back
* Wilander: Basically, noise is another way to make sure a report can't be linked to a particular user or event.  We recognize that these limited sets of bits, we're asking advertisers to map their businesses needs into a set number of bits — so what gets reported isn't a linear thing; adding 1 to the conversion value may mean something completely different.  So we concluded that noise is hard to define.
* Charlie: Good thing to talk about.  The way I think about noise is exactly in terms of looking at aggregates, not individuals.  If a report from the API has metadata 6 instead of 5, then I expect sophisticated adtech to not give credence to the individual report, but to look at aggregates and form statistical distributions — what is the distribution of report types over the last day?  Get confidence intervals of what you expect.  Same idea applies to Ben's proposal for adding noise: once you group events together, then you learn statistical distributions of these groups.  The benefit of event-level noise is "late binding" — you don't need to pack everything into 6 bits, instead at analytics time you get to ask whatever questions you want after-the-fact, and event-level data lets you do that rich query surface of analysis (but with more restrictions, you keep privacy by getting more noise)
* Wilander: I increasingly see that it's on the click side or view side where people want granular data — but I don't know if that's true; we really need agreement on that from all parties if that's the way we're going to go.  In that case we'd be willing to revisit PCM structure more towards that.
* Ben: Measurement is one of the two use cases that are really important to us — fine-grained slicing, valuable to advertisers.  But that's not the only important use case; the other is optimization: we have millions of ads to select from, want to find one that is more likely to be relevant.  Here "relevancy" isn't about retargeting ads; we want people to look at what people interacted with what ads, in order to make group inferences about large groups of people.  Granular data, at an event level, isn't just about reporting, it's input to an ML algorithm for training.  Our research: If you add noise to that data (add in false positives, false negatives), then as long as the noise doesn't blow out the signal-to-noise ratio, it still has value for ML training, that's the useful case.  I like that the Chrome Privacy Sandbox has split out those two use cases: aggregate reporting API handles the reporting use case; event-level API, even with false positives and negatives, still allows the optimization / ML training use case.  This just isn't possible with the Safari 6-bits-on-the-event approach
* John Sabella: Agreed with Ben: individual events are just not of use; there are too many.  It's all about aggregation and being able to learn things at scale.  Other than folks trying to do retargeting, where noise might break them, a version with noise is great as long as we can get signal
* Valentino: You're always trying to understand the patterns of thousands, or tens of thousands, of users.  All the data entering our ML algorithms is a hashed derived signal from what we get; the cookie doesn't even enter the ML algorithm.  If we can figure out how to maintain event granularity, especially sequence of events, and leaving it as clusters of users, that would be extremely useful
* John Sabella: Charlie: your proposal involved delaying the data is a big problem for us, though.
* John WIlander: Thank you Ben, that was helpful — We are currently only interested in supporting the measurement case — we don't want a browser technology to support reasoning about users across sites, even at a group level.  Nothing is final, but we deliberately are trying to solve the measurement case, not the optimization case.  This is why we are having a hard time aligning.  We're more likely to look at Google's aggregate proposal, rather than a noisy event-level proposal
* Andrew Knox (FB): That's helpful to hear.  One thing this hinges on (noise vs. limited bits) is what it means to be private.  At the end of the day, privacy is about deniability for the user — so they can deny that some fact about them is true
* Wilander: Wait, I think autonomy should be part of it — being free from ML being applied to you personal traits to push you in various directions
* Charlie: But even PCM allows this, just for large cohorts
* Wilander: There is a trade-off, but privacy is not just about deniability
* Ben Savage: Can you go into your rationale for why that is not something you want to support?  The optimization use case: as we reported in web-adv BG, just removing personalization from ranking destroyed monetization (-50%), even if all measurement is permitted.  Businesses are made out of people; when they run ads, they are just trying to connect with people, find prospective customers.  When you're a person seeing ads: if the ad has no relevance to me, it has no value, I just scroll past it; when I see an ad that has some connection to my life, I find value in that.  If I have 100 ad slots on my site, I can show random ads, or more relevant ads.  What is the down-side of relevance / of having more successful connections made?
* Tanvi Vyas (Mozilla): The downside is that you can target people, manipulate them, in ways that compromise democracy, in ways where you realize things about people that is more than they know about themselves
* Wilander: Agreed — news bubbles, radicalization, anti-vaxers, etc
* Aram Z-S (Washington Post): We're not comfortable with that 50% revenue loss number
* Paul Bannister: As a publisher, I see data very similar to what Ben mentioned!  I'm worried about the same things that John and Tanvi mentioned, but a web that's dead is bad also.  Can we find a happy medium?  Avoid radicalization and risk to democracy, but still let sites make revenue
* Ben Savage: I understand, and I hear your concerns about systems that know too much, I don't want that either.  I think we're conflating some very different things: vaccinations and filter bubbles are not related to advertising ecosystem problems that we're trying to solve.  People like confirmation bias. That part is about people's choices, people choosing what to consume, and that is not part of display advertising, which is all about selling things.  I understand concerns about retargeting ads; as an individual, I really dislike them, I won't die on that hill.  Maybe retargeting is "knows too much".  But totally random ads goes too far the other way.  Happy medium, event-level data with lots of noise added, the machine is not going to learn anything about you in particular.  But over lots of people, it might lead to better ads.
* John Sabella: In our world we've very programmatic-oriented — the signals we promise are really minimal, we're open-web-oriented, tiny compared to walled-garden signals.  I would hope that we can reconsider the happy-medium case
* Mark X: Illuminating for me too.  I see that users want to establish that relationship.  What makes an ad relevant?  There is some information revealed, to determine why a particular ad might be useful to me — or why misinformation or radical content might be relevant to me could be useful to the publisher.  Attempts to make transparency, let the user see what they are revealing, is helpful
* Andrew Knox: Study Participation Harm — shouldn't be able to run studies where participating brings you harm, even if the overall results of the study might in general (e.g. smoking vs insurance — the fact that you are in the study shouldn't hurt you, even if the result that smoking is harmful means everyone who smokes has their insurance rates go up).  If we could exclude you from the study and still get the same result, hard to argue that your privacy was invaded
* Russell Stringham: Most publishers are going to require a login, probably an email address.  I want an advertising model that doesn't force everyone to log in.  We need a solution that is revenue-competitive with login, otherwise that's where everyone is going to move
Seems like a bunch of people need to drop — we will pick this back up in another break-out — probably tomorrow same time.
* From Zoom chat - Christine Runnegar: In case you haven’t seen it, a case example of a switch to contextual advertising, publisher in the Netherlands saw ad revenue go up - https://techcrunch.com/2020/07/24/data-from-dutch-public-broadcaster-shows-the-value-of-ditching-creepy-ads/ 


### Attendees:
1. **Abhinav Sinha (PubMatic)**
1. **Allan Spiegel (Adobe)** - Sr. Quality Engineer, Privacy, Trust, & Governance
1. **Andrew Knox (Facebook)** - Research Scientist - I have a background in fraud prevention and online ad attribution. I’m especially interested in tech/prototypes that create good incentives that benefit individual privacy and the free internet.
1. **Andrew Pascoe (NextRoll)** - Data Science Engineering, Sr. Director. Responsible for ML pipelines and algorithms, and also maintaining user privacy throughout.
1. **Anne van Kesteren (he/him; Mozilla)** — I work on web standards at Mozilla and in this group edit [Storage Partitioning](https://privacycg.github.io/storage-partitioning/).
1. **Aram Zucker-Scharff (The Washington Post, Director of Ad Engineering)**
1. **Arthur Edelstein (Mozilla)** - Product Manager for Firefox Privacy and Security
1. **Basile Leparmentier (Criteo)** Data scientist at Criteo, more involved in the IWABG (with SPARROW), but this group also shapes the future of our discussions on privacy and advertising, so interested to understand point of view and current status.
1. **Ben Savage (Facebook)** - I’m an engineer at Facebook. I work on the intersection of ads and privacy. I care deeply about setting up our ecosystem for long-term health, and helping entrepreneurs grow their businesses. 
1. **Brad Lassey (Google)** Browser developer working on anti-tracking for Chrome
1. **Brandon Maslen (Microsoft)** - Engineer working on Tracking Prevention, SAA, and other networking, storage, and privacy features in Edge. 10. **Brian Campbell (Ping Identity)** - works for a different Ping than PING. Observing today and tomorrow looking to better understand the isLoggedIn stuff and potential impacts 
1. **Charlie Harrison (Google)** — Engineer at Google, working on new privacy preserving APIs, especially focused on measurement. Looking forward to hearing developments of new proposals and to contribute where I can
1. **Chris Pedigo (DCN)** - we’re a trade association representing publishers. Looking to help develop solutions that map to consumer expectations
1. **Chris Wilson (Google)** Web Standards lead, process manager, AC rep. 
1. **Christine Runnegar (PING co-chair)** - Senior Director, Internet Trust at the Internet Society
1. **Christy Harris (Future of Privacy Forum)**
1. **David Benjamin (Google Chrome)** - Chrome engineer working on various security and privacy bits. Previously worked on cryptography and TLS
1. **Devin Rousso (Apple)** - WebKit software engineer, was (until recently) the lead engineer on Web Inspector, interested in new web proposals. Also a member of CSS WG, Browser Testing & Tools WG, and TC39.
1. **Erik Anderson (Microsoft)** - Engineering manager on Microsoft Edge working on platform privacy, networking, and storage. Also W3C Privacy CG co-chair. Excited to move work items forward towards potential standardization.
1. **Harshad Mane (PubMatic)** - Senior Principal Software Engineer
1. **Jack Frankland (unaffiliated)** - Head of front-end development for an ad tech company, but with a personal interest in user privacy
1. **James Hartig (Admiral)**. Co-Founder at Admiral, which helps publishers build relationships with their visitors. Previously worked at Grooveshark, a music streaming startup.
1. **James Rosewell (51Degrees)** - Engineer, Entrepreneur - understand the problems seeking solutions and policy / external input being sought.
1. **John Wilander (Apple WebKit)**. PhD in CS focused on SW security and did many years in appsec before moving to privacy. Lead engineer on Safari’s Intelligent Tracking Prevention. Love the web and have always worked on web things. Hope to move Storage Access API, IsLoggedIn, and bounce tracking prevention forward.
1. **Josh Karlin (Google)** - Engineer Partitioning Chrome’s Storage & Creating Private Ads APIs
1. **Kate Cheney (Apple)** WebKit Privacy Engineer, interested in staying aware of new web privacy proposals.
1. **Kaustubha Govind (Google)** - Browser developer for Chrome Privacy Sandbox. Co-editor of First-Party Sets.
1. **Kristen Chapman (Salesforce)** - Principal Architect, Salesforce Marketing Cloud.  I focus on data privacy for SFMC engineering in particular, and for Salesforce as a whole.
1. **Mark Xue (Apple User Privacy)** - I work on improving user privacy across a number of our products, including Safari, WebKit, and privacy-preserving mechanisms for ad attribution.
1. **Marçal Serrate (Hybrid Theory)**: CTO @ Hybrid Theory, building data-driven advertising products for brands and agencies.
1. **Marshall Vale (Google):** Product Manager for Chrome’s Privacy Sandbox
1. **Melanie Richards (Microsoft)**—Program Manager on Microsoft Edge, with a prior background in web design and front-end development. Working on privacy among a couple other things. Interested in tracking privacy features across the board with interest specifically in identity-related bits.
1. **Michael Kleber (Google Chrome)** — Engineer working on "Chrome Privacy Sandbox", new browser APIs to offer private-by-design ways to accomplish goals that rely on tracking capabilities (like 3p cookies) today.
1. **Mike Lerra (Google)** - PM working on Chrome privacy; focusing on First-Party Sets and Trust Tokens.
1. **Paul Bannister (CafeMedia)** - Head of Strategy at CafeMedia. We support small publishers/bloggers and we’re particularly interested in creating privacy-oriented advertising solutions that support the free, ad-supported open web and small publishers in particular.
1. **Paul Marcilhacy (Criteo)**: Product Manager at Criteo. Involved in the IWABG and working on the SPARROW proposal. Interested in the ad tech ecosystem health and the future of the “free-internet” in general.
1. **Peter Saint-Andre (Mozilla)**, unfortunately here only for the first hour - Sr. Dir. for Strategic Partnerships on the Firefox team 
1. **Raj Belur (Amazon)** - Here to learn about what Industry is working on
1. **Rowan Merewood (Google)** - Developer Relations lead, looking to make proposals and contributing to them accessible for a wide range of web developers.
1. **Russ Hamilton (Google)** - Chrome engineer working on anti-tracking feature development.
1. **Russell Stringham (Adobe)**: I am an architect on Adobe’s Analytics product responsible for data privacy, including support for GDPR, CCPA and similar laws, as well as understanding and responding to privacy restrictions being implemented by the browsers. I am also the architect over consent and preference management for all Adobe Experience products
1. **Sam Weiler (W3C/MIT)** - lead security and privacy work at W3C
1. **Scott Low (Microsoft)** - PM working on privacy and trust on the Microsoft Edge HTML Platform team. Focused on tracking prevention, Storage Access API. Excited to discuss issues and move the web forward!
1. **Sean Bedford (Facebook)**: Engineer working primarily with businesses who use Facebook to advertise. Interested in learning more about the proposals and understanding how they may impact how businesses use the internet to monetise.
1. **Shigeki Ohtsu (Yahoo JAPAN)**- Engineer working on CDN and privacy/security
1. **Shivani Sharma (Google)** - Engineer working on Chrome’s HTTP cache partitioning and on the new fenced frames for cases like privacy-preserving interest based advertising 
1. **Steven Englehardt (Mozilla)** - Privacy Engineer working on anti-tracking projects.
1. **Steven Valdez (Google)** - Chrome Engineer working on new browser APIs in the Privacy Sandbox, particularly Trust Token/PrivacyPass.
1. **Tanvi Vyas** (Mozilla, co-chair) - Principal Engineer, Firefox.  Anti-tracking and privacy features, anti-tracking policy, privacy of Firefox features, formerly web app security. 
1. **Theodore Olsauskas-Warren (Google)** - Chrome engineer working on privacy, specifically performing internal privacy reviews for feature launches with an OWP focus. Looking to get a feel for broader privacy views on features.
1. **Theresa O’Connor (co-chair, Apple)** Standards Lead for the WebKit team & W3C TAG participant. Editing the Storage Access API, hoping to make progress on its open issues!
1. **Valentino Volonghi (NextRoll)**: CTO at NextRoll working on marketing technology and looking forward to understanding more and contributing to the new privacy specs to create a privacy friendly advertising ecosystem, in particular for small advertisers.
1. **Wendy Seltzer (W3C)** - Strategy Lead and Counsel (IAAL), trying to help the web platform incubate new features in a secure and privacy-preserving way.
1. **Jeffrey Yasskin (Google Chrome)** - Privacy Threat Model and Web Packaging
1. **John Sabella (PubMatic)** CTO at PubMatic
1. Hannes
1. Jeddy
1. Jlukas
1. Michael Smith
1. Sam Macbeth (Cliqz/Ghostery)
1. Soujanya
